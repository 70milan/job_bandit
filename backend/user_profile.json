{
  "name": "M",
  "target_positions": [
    "Data Engineer"
  ],
  "summary": "Experienced data engineer ",
  "skills": [
    "na"
  ],
  "resume_text": "Milan Barot\n\nMilanbarot6979@gmail.com - +1(917) 497-6979  -  Fairfax, VA  -  www.linkedin.com/in/mb70\n\nProfessional Summary:\n\nIT professional experience with about 10 years of expertise as a Cloud Database Engineer with Cloud, AWS, Azure, Google, Data architecture, Data Analysis, Data Validation, Date Migration, Transition, Upgradation, Big Data Engineer, ETL Development and Data Analyst including , BI Reporting, designing, developing, and implementing data models for enterprise-level applications. Excellent understanding of technologies on systems that include huge amounts of data and run in a highly distributed fashion in Cloudera, Hortonworks Hadoop distributions, and Amazon AWS. Created an Azure SQL database, monitored it, and restored it. Migrated Microsoft SQL server to Azure SQL database.\n\nExtensive expertise with Amazon Web Services such as Amazon EC2, S3, RDS, IAM, Auto Scaling, CloudWatch, SNS, Athena, Glue, Kinesis, Athena, Lambda, EMR, Redshift, and DynamoDB. Experience with Azure Cloud, Azure Data Factory, Azure Data Lake Storage, Azure Synapse Analytics, Azure Analytical services, Big Data Technologies (Apache Spark), and Data Bricks is preferred. Extensive experience developing and implementing cloud architecture on Microsoft Azure.\n\nDesigned and implemented scalable cloud infrastructure solutions across AWS, Azure, and GCP environments. Managed and optimized existing infrastructure components to ensure high availability and performance. Excellent understanding of connecting Azure Data Factory V2 with a range of data sources and processing the data utilizing pipelines, pipeline parameters, activities, activity parameters, and manually/window-based/event-based task scheduling.\n\nDesigned and implemented data models based on the Common Education Data Standards (CEDS) to support data interoperability and exchange between educational institutions and agencies. Extensive hands-on expertise in Big Data technologies such as HDFS, MapReduce, YARN, Apache Cassandra, NoSQL, Spark, Python, Scala, Sqoop, HBase, Hive, OLAP, Oozie, Impala, Pig, Zookeeper, and Flume. Working knowledge of AWS databases such as ElastiCache (Memcached and Redis) and NoSQL databases such as HBase, Cassandra, and MongoDB, as well as database performance tuning and data modeling. Experience in OLTP/OLAP system study, analysis, and E-R modeling, as well as developing database schemas such as the Star schema and Snowflake schema, which are utilized in relational, dimensional, and multidimensional modeling. \n\nCreated a connection from Azure to an on-premises data center using the Azure Express Route for Single and Multi-Subscription. Developed and maintained data pipelines to extract, transform, and load CEDS-compliant data from various sources into a data warehouse using Apache Spark and AWS Glue. Worked on ETL Migration services by creating and deploying AWS Lambda functions to provide a serverless data pipeline that can be written to Glue Catalog and queried from Athena. Developed ETL pipelines in and out of the data warehouse using a mix of Python and Snowflakes SnowSQL Writing SQL queries against Snowflake.\n\nAnalytics and cloud migration from on-premises to AWS Cloud with AWS EMR, S3, and DynamoDB. Experience in creating and managing reporting and analytics infrastructure for internal business clients using AWS services including Athena, Redshift, Spectrum, EMR, and Quick Sight. Working knowledge in Python programming with a variety of packages such as NumPy, Matplotlib, SciPy, and Pandas. Extensive experience creating Web Services with the Python programming language, including implementation of JSON-based RESTful and XML-based SOAP web services. Experienced in writing complex Python scripts with Object-Oriented principles such as class creation, constructors, overloading, and modules. Experience establishing and maintaining multi-node development and production of Hadoop clusters.\n\nWorked with Spark to improve the speed and optimization of current Hadoop algorithms utilizing Spark Context, Spark-SQL, Data Frame, OLAP, Pair RDD, and Spark YARN. Experience with Hortonworks Ambari in building and maintaining multi-node development and production Hadoop clusters with various Hadoop components (HIVE, PIG, SQOOP, OOZIE, FLUME, HCATALOG, HBASE, ZOOKEEPER). Worked with the Map Reduce programming paradigm and the Hadoop Distributed File System. Experienced Scala in using and spark streaming for ongoing transactions for customers.\n\nExpertise in all aspects of the Software Development Life Cycle (SDLC), including Agile and Waterfall techniques. Participated in workshops and training sessions to stay up-to-date on the latest developments in the CEDS framework and its implementation in educational settings. Works directly with clients and management to resolve conflicts and issues that may arise. Has worked extensively with SQL, including performing Query Optimizations. Design and build common reusable ETL components to define audit processes and capture DQ exceptions, Job monitoring, and ETL logging processes. I worked with MVW frameworks like Django, Angular JS, HTML, CSS, XML, Java Script, jQuery, and Bootstrap. Highly efficient in Java & proficiency in object-oriented programming. Knowledge of various design and patterns in Java and reusable Java libraries.\n\nCreated TypeScript reusable components and services to consume RESTAPI’S using component-based architecture.\n\nAutomated infrastructure provisioning and configuration using Terraform, Ansible, and CloudFormation. Monitored infrastructure health and performance metrics using tools like Prometheus, Grafana, and CloudWatch. Hands on experience building Spark Java and Scala applications for batch and stream processing involving Transformations, Actions, Spark SQL queries on RDD’s, Data frames and Datasets. Expertise in Implementation of SQL Queries, Stored Procedures, Functions, and Triggers. Experienced in Quality stage tool as part of Data Analysis and Data Standardization.\n\nExperience in IBM Infosphere DataStage jobs specification documentation preparation. Experience in working with AWS services like EMR, S3, EC2, AWS Lambda, AWS DynamoDB. Excellent team member with problem-solving and trouble-shooting capabilities, Quick Learner, highly motivated, result oriented and an enthusiastic team player.\n\nExperience in writing python scripts to process bulk data. Good interpersonal skills, experience in handling communication and interactions between different teams. Hands on experience working in healthcare and real estate domains.\n\nSkilled in data warehousing tools, data mapping, unit testing, migration, conversions, and process documentation\n\nExperienced in LAMP (Linux, Apache, MySQL, and PHP/ Python).  Proficient in developing Data Flow Diagrams, Process Models, ER diagrams, Dimensional Data Models, Ad-hoc reports using Business Objects. Good knowledge in working and implementation of Crystal Reports 8.0/9.0/10/Xi. Expertise in installing, designing, developing, maintaining and tuning of RDBMS like SQL 2012, 2016. Experience with Production (24 X 7) and Development Environment. Experience with 2016 new concepts like Date Time data type, Transparent Data Encryption, Grouping sets, MERGE operator, Change Data Capture, GEOGRAPHY and GEOMETRY Data Types, Filtered Indexes, Data Profiling, SQL Server Integration Services(SSIS), SQL Server Reporting Services (SSRS)and SQL Server Analysis Services (SSAS).\n\nHighly proficient in the use of T-SQL for developing complex creating Database, Tables, Indexes, Views, Triggers, Stored Procedure, User Defined Function, relational database models and data integrity, SQL joins and query writing. Experienced in Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics and Excel data extracts. Experience with 2016 new concepts like data partitioning, online indexing, Data Definition Language (DDL) triggers, Dynamic Management views (DMV), Dynamic Management Functions (DMF), SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS) and SQL Server Analysis Services (SSAS).\n\nExtensive experience in Extracting, Transforming and Loading (ETL tools) using SSIS, DTS Import/Export Data, Bulk Insert, BCP, DTS Packages and SSIS packages. Excellent experience of SQL Server DTS and SSIS (Integration Service) package design, constructing and deployment. Experience in MS SQL Server 2012, 2016 Business Intelligence in MS SQL Server Integration Services (SSIS), MS SQL Server Reporting Services (SSRS), and MS SQL Server Analysis Services (SSAS).\n\nExperience in using SSIS tools like Import and Export Wizard, Package Installation, and SSIS Package Designer.\n\nSkilled in data warehousing tools, data mapping, unit testing, migration, conversions, and process documentation\n\nExperience in SQL Server 2016, 2012 SQL, T-SQL, Stored Procedures, Triggers, UDF, BCP, DTS, SSIS packages, PL/SQL, SQL Plus and Reports. Experienced working towards building a strong team/work environment, and have the ability to acclimate to new technologies and situations with ease. Highly motivated team player with excellent analytical, problem solving, interpersonal and communication skills.\n\n  TECHNICAL SKILLS:\n\nProfessional Experience: -\n\nCLOUD Engineer/Application Developer - Mastek for Blue Cross Blue Shield - Phoenix, AZ \n\n(Healthcare/ insurance)                             \n\nTrading Partner Data Management                                                                                                                        Sep 22 – current\n\nkEY ROLES AND RESPONSBILITIES: \n\nDesigned and implemented end-to-end ETL pipelines using SSIS and SQL Server to process eligibility, claims, accumulator, and value-based care (VBC) files, ensuring compliance with business requirements and vendor-specific standards. Collaborated with vendors such as OptumRx, HealthEquity, Equality Health, and others to deliver high-quality data solutions.\n\nPlayed a key role in the migration of legacy system members to HealthRules Payer (HRP) by developing scalable pipelines to integrate data and transitioning existing processes from legacy systems to HRP. Generated files from HRP while maintaining compatibility with legacy processes for seamless operational continuity.\n\nPartnered with business stakeholders to gather requirements for new projects, conducting detailed assessments of complexity, technical effort, development timelines, and warranty periods. Worked with Mastek to draft SOWs for approval by Blue Cross Blue Shield, ensuring alignment with business objectives. Collaborated closely with Deloitte on the HealthEquity project to develop eligibility and claims files from scratch. Responsibilities included gathering business requirements, creating technical documentation, optimizing ETL pipelines, conducting performance testing, and ensuring smooth deployment into production environments.\n\nAutomated data workflows by integrating SQL Server Agent jobs with ActiveBatch, setting up FTP connectivity with strict naming conventions and cadence-driven schedules for file delivery to vendors. Generated and processed EDI 834 enrollment files, facilitating secure data exchange through EDI tools while maintaining robust documentation for all processes to ensure audit readiness. Performed infrastructure cost analysis and implemented strategies to reduce cloud expenditure. Migrated legacy on-premises systems to modern cloud infrastructure environments.\n\nConducted performance reviews of legacy pipelines, refactoring and optimized code upto 50% for improved scalability and efficiency while adhering to best practices in data engineering. Developed an ASP.NET web application executing CRUD operations on an Oracle database, eliminating manual data entry and significantly improving data accuracy. \n\nDesigned and deployed interactive and dynamic dashboards using Microsoft’s SSRS with Drill Down, Drill Through, and Drop-down menu options and parameterized reports, enabling stakeholders to access real-time insights and make data-driven decisions. Oversaw a small team of offshore resources by recruiting and onboarding team members as required for project execution. Ensured tasks were completed on time by maintaining close collaboration with both onshore and offshore teams without directly leading the team.\n\nParticipated in Agile sprint meetings to track progress on deliverables, resolve blockers, and ensure timely completion of project milestones across multiple initiatives. \n\nExtensively used AWS Athena to import structured data from S3 into other systems such as Red Shift or to generate reports.\n\nWorked with Spark to improve the speed and optimization of OLAP, Hadoop's current algorithms.\n\nMigrated an existing on-premises application to AWS. AWS services such as EC2 and S3 were used for data set processing and storage. Experienced in maintaining a Hadoop cluster on AWS EMR. Maintained infrastructure security by implementing IAM policies, encryption, and network access controls. Conducted disaster recovery and backup planning for mission-critical infrastructure.\n\nCreated a Data Pipeline utilizing Processor Groups and numerous processors in Apache Nifi for Flat File, RDBMS as part of a Proof of Concept (POC) on Amazon EC2. Migrated an existing on-premises application to AWS. AWS services such as EC2 and S3 were used for data set processing and storage. Experienced in maintaining a Hadoop cluster on AWS EMR.\n\nThe Spark-Streaming APIs were utilized to perform on-the-fly transformations and actions for the common learner data model, which gets data from Kinesis in near real-time. Performed end-to-end architecture and implementation evaluations of different AWS services such as Amazon EMR, Redshift, OLAP, S3, Athena, Glue, and Kinesis. Hive We created external table schemas for the data being processed as the primary query engine of EMR.\n\nCreated Apache presto and Apache drill configurations on an AWS EMR (Elastic Map Reduce) cluster to integrate different databases such as MySQL and Hive. This allows for the comparison of outcomes such as joins and inserts on many data sources controlled by a single platform. Designed and implemented data models based on the Common Education Data Standards (CEDS) to support data interoperability and exchange between educational institutions and agencies\n\nAWS RDS (Relational database services) was created to act as a Hive meta store, and metadata from 20 EMR clusters could be integrated into a single RDS, preventing data loss even if the EMR was terminated. Developed and implemented ETL pipelines on S3 parquet files in a data lake using AWS Glue. Developed a cloud formation template in JSON format to utilize content delivery with cross-region replication using Amazon Virtual Private Cloud.\n\nThe AWS Code Commit Repository was utilized to preserve programming logic and scripts, which were subsequently replicated to new clusters. Worked closely with educational data analysts and scientists to ensure that the data models and pipelines met the requirements of the CEDS framework and supported their research and analysis needs\n\nImplemented Columnar Data Storage, Advanced Compression, and Massive Parallel Processing using the Multi-node Redshift technology. Developed and maintained data pipelines to extract, transform, and load CEDS-compliant data from various sources into a data warehouse using Apache Spark and AWS Glue. Supported and troubleshot infrastructure issues across multiple environments in real time. Built and maintained infrastructure as code (IaC) repositories for consistent environment provisioning. Created detailed documentation for infrastructure design, processes, and operational procedures. Ensured compliance and governance standards were met across all infrastructure components.\n\nInvolved in the development of the new AWS Fargate API, which is comparable to the ECS run task API. Worked on the code transfer of a quality monitoring application from AWS EC2 to AWS Lambda, as well as the construction of logical datasets to administer quality monitoring on snowflake warehouses.\n\nProficient with container systems such as Docker and container orchestration tools such as EC2 Container Service, Kubernetes, and Terraform. Worked on creating workloads HDFS on Kubernetes clusters to mimic the production workload for development purposes. Deployed Kubernetes pods using KUBECTL in EKS.Expert in implementing advanced procedures like text analytics and processing using in-memory computing capabilities like Apache Spark written in Scala\n\nCoordinated infrastructure upgrades, patching, and maintenance to ensure system stability. Implemented container-based infrastructure using Kubernetes, Docker, and Helm for application deployment. Configured Spark Streaming to receive real-time data from Apache Kafka and store the stream data to HDFS using Scala.\n\nParticipated in workshops and training sessions to stay up-to-date on the latest developments in the CEDS framework and its implementation in educational settings.\n\nEnvironment: MS SQL Server, SSIS/SSRS, Power BI, ASP.NET, ActiveBatch, Azure DevOps, GIT, Python, PySpark, Kafka, Reltio, GitLab, PyCharm, AWS S3, Snowflake. Cloudera CDH 5.9.16, Hive, Impala, Kubernetes, Flume, Apache Nifi, Java, OLAP, Shell-scripting, SQL, Sqoop, Oozie, Java, Python, Oracle, SQL Server, HBase, PowerBI, Agile Methodology.\n\nCLOUD Engineer/Application Developer – PRODUCTION SUPPORT  - CBRE, Dallas, TX, \t                 Nov 20 – Sept 22\n\nKEY ROLES AND Responsibilities:\n\nIdentified, designed, and implemented internal process improvements such as automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability. This resulted in significant annual cost savings for the organization and improved data processing efficiency by 20%. Built Validation Framework that would compare the data at source RDBMS Systems and Destination Hive tables. Analysing large amounts of data sets to determine optimal way to aggregate and report on these data sets. Designed and Implemented Big Data Analytics architecture/pipeline.\n\nProven experience in setting up comprehensive monitoring and alerting systems using Amazon CloudWatch, ensuring real-time insights into application and infrastructure performance. Designed, developed ODI (Oracle Data Integrator) (Data Integration) objects for ETL Project, gathering requirement specification documents and presenting and identifying data sources. Developed data pipelines for ETL from sources’ APIs, deployed in AWS S3 with scheduler, loaded it to PostgreSQL sever to further perform transformations to derive pertaining datasets for Tableau reports and dashboard creation.    \n\nUsed Python as programming Language, PostgreSQL as the database storage, Mesos to manage the Azure cloud infrastructure, Jenkins for Continuous integration and deployment. \n\nDeployed monitoring and alerting systems to enhance infrastructure observability and reliability. Conducted performance tuning and optimization for cloud infrastructure resources. Competent in container orchestration using Amazon ECS (Elastic Container Service), streamlining the deployment and scaling of containerized applications. Experienced in setting up event-driven architectures with Amazon Simple Notification Service (SNS), enabling efficient communication and notification workflows. Supported incident response and root cause analysis related to infrastructure outages. Led initiatives to modernize and standardize enterprise infrastructure architecture.\n\nFamiliarity with Python's ecosystem for data science and machine learning, enabling the development of data-driven applications and predictive models. Created ODI (Data Integration) design documents from the existing Informatica mappings. Used these design documents in development of ODI interfaces/packages.\n\nIdentified, designed, and implemented internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. \n\nContributed to the maintenance and revamping the “Lymbic” framework (ETL tool), using Python and SQL. Created plugins in Python, customized for downloading and processing files in various formats. Performed in agile methodology, interacted directly with entire team provided/took feedback on design, Suggested/implemented optimal solutions, and tailored application to meet business requirements.\n\nDevelop DataStage jobs which Extracts, Transforms and Load data to required tables in the Netezza database. Develop complete front-end stack of projects using HTML/CSS and JavaScript with AngularJS. Used HTML and JavaScript to develop new user interfaces for applications. Integrate multiple pages into a single-page application using AngularJS.\n\nStreamlined infrastructure workflows by integrating CI/CD, version control, and automation pipelines. Supported blue-green and canary deployment models to modernize infrastructure operations. Evaluated new tools and technologies to enhance infrastructure automation and observability. Managing and supporting production environments. Coordination with different teams for planning and executing activities during software release and server switchover. Resolved users’ technical issues by providing quick solutions to bugs through continuous code updates. Optimized data sources and processing rules to enhance data quality through design and development phases. Drafted technical documentation for internal business areas and processes, incorporating factors such as technical design, data manipulation, ETL and storage management.\n\nDetermined data storage and optimization policies, shaping organization efforts to enhance performance. Implemented Apache Airflow for authoring, scheduling, and monitoring Data Pipelines. Designed several DAGs (Directed Acyclic Graph) for automating ETL pipelines. Performed data extraction, transformation, loading, and integration in data warehouse, operational data stores, and master data management \n\nPerformed Data Migration to GCP, Responsible for data services and data movement infrastructures. Experienced in ETL concepts, building ETL solutions, and Data modelling. Worked on architecting the ETL transformation layers and writing spark jobs to do the processing. Aggregated daily sales team updates to send a report to executives and organize jobs running on Spark clusters. Loaded application analytics data into the data warehouse at regular intervals of time \n\nDeveloped runbooks and incident response procedures for infrastructure monitoring teams. Built high-availability clusters and fault-tolerant infrastructure components for mission-critical systems. Designed & build infrastructure for the Google Cloud environment from scratch \n\nExperienced in fact dimensional modeling (Star schema, Snowflake schema), transactional modeling, and SCD (Slowly changing dimension). Leveraged cloud and GPU computing technologies for automated machine learning and analytics pipelines in OLAP, GCP. Worked closely with educational data analysts and scientists to ensure that the data models and pipelines met the requirements of the CEDS framework and supported their research and analysis needs\n\nAutomated user provisioning and configuration management within the infrastructure ecosystem. Enabled zero-downtime deployment strategies for production infrastructure. Developed and maintained data pipelines to extract, transform, and load CEDS-compliant data from various sources into a data warehouse using Apache Spark and AWS Glue. Designed and implemented a configurable data delivery pipeline for scheduled updates to customer-facing data stores built with Python. Proficient in Machine Learning techniques (Decision Trees, Linear/Logistic Regressors) and Statistical Modeling. Compiled data from various sources to perform complex analysis for actionable results. Experience in working with different join patterns and implementing both Map and Reduce Side Joins. \n\nWrote Flume configuration files for importing streaming log data into HBase with Flume. Imported several transactional logs from web servers with Flume to ingest the data into HDFS.  Using Flume and Spool directory for loading the data from the local system (LFS) to HDFS.  Installed and configured pig, written Pig Latin scripts to convert the data from the Text file to Avro format. Created Partitioned OLAP and Hive tables and worked on them using HiveQL. \n\nLoading Data into HBase using Bulk Load and Non-bulk load. Worked on continuous Integration tools Jenkins and automated jar files at end of the day.  Worked with Tableau and Integrated Hive, Tableau Desktop reports and published to Tableau Server. \n\nExperience in setting up the whole app stack, setting up and debugging log stash to send Apache logs. Developed Spark code using Scala and Spark-SQL/Streaming for faster testing and processing of data.  Used Spark-SQL to Load JSON data and create Schema RDD and loaded it into Hive Tables and handled structured data using Spark SQL. \n\nImplemented Spark Scripts using Scala, Spark SQL to access hive tables into Spark for faster processing of data. Validated infrastructure performance using load testing and benchmarking tools. Participated in architecture reviews to ensure infrastructure met disaster recovery objectives.\n\nTested Apache Tez for building high-performance batch and interactive data processing applications on Pig and Hive jobs. \n\nMeasured Efficiency of the Hadoop/Hive environment ensuring SLA is met. Optimized the TensorFlow Model for efficiency, Design star schema in Big Query. Analyzed the system for new enhancements/functionalities and performed Impact analysis of the application for implementing ETL changes. Implemented a Continuous Delivery pipeline with Docker, and Git Hub\n\nBuilt performant, scalable ETL processes to load, cleanse and validate data. Participated in the full software development lifecycle with requirements, solution design, development, QA implementation, and product support using Scrum and other Agile methodologies. Collaborate with team members and stakeholders in the design and development of the data environment Tracked and leveraged data trends, aggregate statistics and volume fluctuations to hone performance metrics.\n\nIntroduced observability best practices into the infrastructure lifecycle management process. Optimized DNS, CDN, and edge infrastructure configurations for global user access. Collaborated with internal and external stakeholders to optimize data sourcing pipelines and verify data quality.\n\nActed as liaison between clients and technical associates, translating complex ideas into easy-to-understand explanations. Generated standard and custom reports to provide insights into business performance. Optimized data access and storage to improve performance of analytics systems. Developed and maintained data warehouses and data marts to support business operations. Provided technical support for troubleshooting analytics and reporting issues. Implemented business intelligence solutions to increase operational efficiency. Developed and implemented data governance policies and procedures. Designed and developed data pipelines to acquire, clean and process data. Updated and developed scripts and queries to extract and analyze data from multiple sources. Identified patterns and trends in large data sets and provided actionable insights.\n\nEnvironment: AWS, Python, Jenkins, Json, Postgres, Snowflake, Data Infrastructure Development, Automation, Jenkins, Docker, Python, Jenkins, Technical Documentation, Java, python 2.7, DataStage, Hadoop, S3, DynamoDB, EC2, Kinesis, KVS, SNS, SQS, IAM, Rekognition, JavaScript, Angular JS, HTML, CSS, Visual Studio, IBM DB2, Netezza, Oracle, Unix, Squirrel Client, Control M, Aws, Big Data.\n\nCLOUD Engineer/Application Developer – PRODUCTION SUPPORT - National MI, New York, NY   Oct 18 – Feb 20\n\nKEY ROLES AND Responsibilities:\n\nLoad Data From On Premise DB MY SQL to Snowflake Data Warehouse and Performing Data. Manipulation using Snow SQL\n\nExperienced in writing Spark Applications Python. Built multiple notebooks and piped all of the jobs together, built an ETL pipeline to making wear algorithm predictions of the model, and writing the outputs to AWS Redshift. Analysed the SQL scripts and designed the solution to implement using Spark. Developed custom aggregate functions using Spark SQL and performed interactive querying. Implemented ActiveBatch job scheduler for authoring, scheduling and monitoring Data Pipelines\n\nDeveloped Python code to gather the data from API’s and designs the solution to implement using Spark. Implemented Kinesis model which pulls the latest records into S3 Bucket. Imported data into ADLS from various API’s and SQL databases and files using ETL and from streaming systems. Loaded all data-sets into Hive from Source CSV files using spark from Source CSV files using Spark. Exported the analysed data to Teradata using Sqoop for visualization and to generate reports for the BI team.\n\nMigrated the computational code in HQL to PySpark. Completed data extraction, aggregation and analysis in HDFS by using PySpark and store the data needed to Hive. Experienced in fact dimensional modelling (Star schema), transnational modelling and SCD (Slowly changing dimension). Working on the integration of data engineering and Tire Wear model with data science team, to make the correct output predictions and working on fixing other issues.\n\nPartnered with security teams to perform vulnerability assessments on infrastructure assets. Designed backup and replication strategies for critical infrastructure data and services. Sound knowledge in programming Spark using Scala. Extract Transform and Load data from Sources Systems to AWS RDS services using a combination of, T-SQL, Spark SQL. Data Ingestion to one or more AWS Services - (AWS RDS, AWS Redshift) Experienced in Importing and exporting data into HDFS and Hive using Sqoop. Deploying Spark jobs in Amazon EMR and running the job on AWS clusters. Interacting with the customer/Business Analyst on requirement gathering\n\nDeveloped an ASP.NET web application using Framework 6 that performed CRUD operations on a SQL Server database, eliminating the need for manual data entry and a significant increase in data accuracy. Designed and deployed interactive and dynamic dashboards using SSRS with Drill Down, Drill Through and Drop-down menu option and parameterized reports, enabling stakeholders to access real-time insights and make data-driven decisions. Collaborated with ETL team, analyzed workflow, and made recommendation for batch processing using SQL packages. Improved functionality and feature development both within current application framework and on future generation frameworks. Provided technical support to clients daily, Contribute to servlet-based application development. \n\nAssist in maintaining and updating existing applications and modules, Help design form validation programs using HTML and JavaScript. Contribute to development of client-side and server-side codes for external and internal web applications. Worked with data transfer from on-premises SQL servers to cloud databases (Azure Synapse Analytics (DW) and Azure SQL DB). Created Pipelines that were built in Azure Data Factory using Linked Services/Datasets/Pipeline/ to extract, transform, and load data from a variety of sources including Azure SQL, OLAP, Blob storage, Azure SQL Data warehouse, write-back tool, and reverse. Created CI-CD Pipelines using Azure DevOps, Used a blend of Azure Data Factory, OLAP, T-SQL, Spark SQL, and U-SQL Azure Data Lake Analytics, to gather, convert, and load the data from source systems to Azure Data Storage services. Processed structured and semi-structured data into Spark Clusters using Spark SQL and DataFrames API. Created Spark apps with Azure Data Factory and Spark-SQL for data extraction, transformation, and aggregation from various file formats to analyze and transform the data in order to reveal insights into consumer usage patterns.\n\nEstablished version control standards for infrastructure configuration files and scripts. Oversaw migration of workloads to cloud-native infrastructure services to improve scalability. Ingestion of data into one or more Azure Services (Azure Data Lake, Azure Storage, Azure SQL, Azure DW) and processing of data in Azure Databricks. Monitored the SQL scripts and modified them for improved performance using PySpark SQL.\n\nManaged relational database service in which Azure SQL manages scalability, stability, and maintenance. Integrated data storage options with Spark, notably with Azure Data Lake Storage and Blob storage. Configured stream analytics, Event Hubs, and worked with Azure to manage IoT solutions. Successfully executed a proof of concept for Azure implementation, with the wider objective of transferring on-premises servers and data to the cloud. Proven track record of optimizing Spark application performance for optimal batch interval time, parallelism level, and memory optimization. Analyzed data quality issues using OLAP, Snow SQL by creating analytical warehouses on Snowflake. Created UDFs in Scala and PySpark to satisfy specific business requirements. Used Hive queries to analyze huge data sets of structured, unstructured, and semi-structured data.\n\nSupported continuous improvement of infrastructure processes through automation and scripting. Conducted regular audits to identify gaps and inefficiencies in the existing infrastructure. Used structured data in Hive to enhance performance using sophisticated techniques including bucketing, partitioning, and optimizing self-joins. Used Kubernetes for the runtime environment of the CI/CD system to build, test deploy.\n\nUsed Kubernetes to manage Docker orchestration and containerization. Used Kubernetes to orchestrate the deployment, scaling, and management of Docker Containers.\n\nInvolved in developing a linear regression model to predict a continuous measurement for improving the observation of wind turbine data developed using spark with Scala API. Used Spark and Spark-SQL to read the parquet data and create the tables in the hive using the Scala API. Using AngularJS, created custom directives for data manipulations and to display data in company standard systems life cycle.\n\nManaged identity and access management (IAM) policies to secure infrastructure resources. Troubleshot infrastructure latency and connectivity issues across distributed systems. Analyzed and edited existing computer applications to improve functionality. Participated in design and planning exercises for future software rollouts. Explained project technical risks and benefits during project kick-off. Assisted with creation of user manuals. Collaborated with stakeholders regarding project capabilities and limitations to deliver optimal functionality.\n\nWrote code for database applications. Collaborated with multidisciplinary teams to design and implement new technology features. Developed and implemented personalized client solutions using programming language expertise.\n\nDelivered support training to help customers learn key features of applications. Liaised with the sales department to identify technical requirements and developed solutions. Installed applications and confirmed proper communication between systems and equipment. \n\nEnvironment: AWS, Spark, Python, Snowflake, Jenkins, Airflow, UNIX, ActiveBatch, LabVIEW, Python, Unix Scripting, SQL server, HTML, JavaScript, Java, HTML, CSS, Angular JS, Bootstrap, Visual Studio, AWS, DynamoDB, S3.\n\ncloud Engineer/Application Developer – PRODUCTION SUPPORT - WebMobi Technologies, Vadodara, IND/New \n\nBrunswick New Jersey\t\t\t                                                                                                               Mar 14 -Oct 18\n\nKEY ROLES AND Responsibilities:\n\nSQL Server 2012 RDBMS database development using T-SQL programming, queries, Stored Procedures, Views. Worked on Extracting, Transforming and Loading (ETL) process to load data from Excel, Flat file to MS SQL Server using SSIS. Created SSIS packages to load the data from Text File to staging server and then from staging server to Data warehouse.\n\nETL implementation using SQL Server Integration Services (SSIS), Applying some business logic and data cleaning in a staging server to maintain child and parent relationship. Worked on Control flow tasks such as Execute SQL task, Send Mail Task, File System Task, Data Flow Task and used different data sources and destination with derived column, lookup transformation within Data Flow Task.\n\nExperience in creating SSIS packages using different type’s tasks, with Error Handling, Log particular event by creating logging and working on package configuration to run packages on different servers. Created Database and Database Objects like Tables, Stored Procedures, Views, Triggers, Rules, Defaults, user defined data types and functions.\n\nDeployed infrastructure-as-code (IaC) templates to standardize infrastructure provisioning across projects. Designed network topologies and infrastructure blueprints to meet high-performance requirements. Adept at infrastructure as code (IaC) using AWS CloudFormation, automating the provisioning and management of AWS resources for rapid and consistent deployments. Automated repetitive tasks such as data transformation and cleaning, resulting in a 40% reduction in manual effort and improved data consistency.\n\nDeveloped an ASP.NET web application using Framework 6 that performed CRUD operations on a SQL Server database, eliminating the need for manual data entry and a significant increase in data accuracy Troubleshoot and resolve database-related issues, including database connectivity, security, and performance issues. Collaborate with cross-functional teams to gather business requirements, analyse data and provide solutions for business problems.  Develop a server environment where users are placed in a queue and only one user is allowed access to the hardware. \n\nEnhanced infrastructure resilience by implementing multi-region and failover strategies. Validated infrastructure compliance with organizational and industry security standards. Good understanding and experience in implementation of JavaScript design patterns and frameworks (AngularJS, Angular). Modify the LabView programs suitable to the server program. Wrote and executed various MYSQL database queries from python using Python-MySQL connector and MySQL dB package.\n\nCreated a Power BI data model based on analysis of the end-user workflow data provided by the client. Imported data from SQL Server DB, and Azure SQL DB to Power BI to generate reports. Developed analysis reports and visualization using DAX functions like table, aggregation, and iteration functions. Optimized data collection procedures and generated reports weekly, monthly, and quarterly. Designed, developed, tested, and maintained Tableau functional reports based on client requirements\n\nPresented application to Hospital’s Executive Team and put the application into a production system for them to use SQL & Tableau Projects. Produced custom visualizations for the client to monitor all patient registration-related workflow, thus giving excess to their supervisor team to monitor high error rate departments\n\nExtracted data from SQL Server database tables into flat data files and Excel sheets using table export and BCP for easy data migration. Developed advanced SQL queries with multi-table joins, group functions, subqueries, set operations, and T- SQL stored procedures, user-defined functions (UDFs) for data analysis\n\nSupported hybrid infrastructure solutions connecting on-premises data centers with cloud environments. Implemented CI/CD automation to deploy infrastructure updates with minimal downtime. Involved in designing physical and logical data models using the ERwin Data modeling tool. Designed the relational data model for operational data store and staging areas, Designed Dimension & Fact tables for data marts. Extensively used ERwin data modeler to design Logical/Physical Data Models and relational database design. Created Stored Procedures, Database Triggers, Functions, and Packages to manipulate the database and apply the business logic according to the user's specifications.\n\nIntegrated monitoring tools into the existing infrastructure to improve visibility and uptime. Configured network infrastructure components including VPCs, subnets, firewalls, and load balancers. Created Triggers, Views, Synonyms, and Roles to maintain integrity plan and database security. Creation of database links to connect to the other server and access the required info. Integrity constraints, database triggers, and indexes were planned and created to maintain data integrity and to facilitate better performance. Used Advanced Querying for exchanging messages and communicating between different modules. System analysis and design for enhancements Testing Forms, Reports and User Interaction.\n\nEnvironment: SQL Server 2012/2016, Server Integration Services 2012/2016, SQL server Reporting Service 2012/2016, Microsoft Visual Studio, SQL Server Management Studio,  MS Excel, T-SQL,ERWIN 7.2, C# .Net, SharePoint , TFS, Unix Scripting, MySQL, Angular JS, Microsoft Power BI, Tableau, SQL, Excel, Agile/Scrum, Oracle 9i, SQL* Plus, PL/SQL, ERwin, TOAD, Stored Procedures.\n\nEducation:\n\nGujarat Technological University\t-2015 \n\nBachelor’s in Computer Applications\t\t\t                                                         \t\t    ",
  "job_description": "asdsda"
}